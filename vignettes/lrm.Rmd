---
title: "LogisticReg Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lrm}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Introduction to LogisticReg

LogisticReg is an R package perform logistic regression on binary outcome. This mimic the function glm() from stats package but only focus on the binomial family with a logit link. $$logit(P(Y=1)) = X\beta$$


In this package, what you can do includes but not limited to:  

1. Fit logistic regression with or without intercept  

2. Get estimated coefficients  

3. Evaluate your model fitting by deviance and $R^2$  

4. Predict outcome with new data

# Tutorial 

First, let's load the package
```{r}
library(LogisticReg)
```

We'll use a simulated dataset of 100 samples, 10 predictors to show the way of using lrm() and compare it with the results from glm().
```{r}
set.seed(1)
y = sample(c(0, 1), 100, replace = TRUE)
X = matrix(rnorm(1e4), 100, 10)
```

## Fit Models

Now fit model with intercept in both glm() and lrm(). We set the same initial value to achieve same results from Iteratively Reweighting Least Square iterations.
```{r}
# fit glm
glm.fit = glm(y ~ X, family = binomial, start=c(rep(0, 11)))

# fit lrm
lrm.fit = lrm(y, X)
```


## Check Results
lrm() returns a list with the data and the information from fitted model. We do not directly type lrm.fit to print but use names(lrm.fit) to check the information in the list.
```{r}
names(lrm.fit)
```

Let's check if lrm() and glm() give the same coefficients and fitted probabilities.  

```{r}
all.equal(unname(glm.fit$coefficients), as.vector(lrm.fit$coef))
all.equal(unname(glm.fit$fitted.values), as.vector(lrm.fit$prob))
```
There are more information provided for you to understand the model.  
```{r}
lrm.fit$loglik
lrm.fit$deviance
lrm.fit$aic
```

In addition, we can calculate Cox & Snell	$R^2$ value and the adjusted value of it to understand the goodness of fit.
```{r}
lrm.rsqr(lrm.fit)
```

To predict new data, we can use lrm.predict() with a input matrix of new data and the list returned from lrm().  
```{r}
Xnew = sapply(1:10, function(i)rnorm(3, mean(X[,i]), sd(X[,i])))
lrm.predict(Xnew, lrm.fit)
```

We can do the same thing with non-intercept model.  For simplicity we won't check every results here again.
```{r}
glm.fit = glm(y ~ -1 + X, family = binomial, start=c(rep(0, 10)))
lrm.fit = lrm(y, X, intercept = FALSE)
```


# Efficiency

We can also compare the run time of lrm() nad glm()
```{r}
compare_result = bench::mark({
  lrm.coef = unname(as.vector(lrm(y,X)$coef))
}, {
  glm.coef = unname(glm(y ~ X, family = binomial, start=c(rep(0, 11)))[["coefficients"]])
})
compare_result[, 3]
```
We see the the lrm() performs slower than glm() but it is comparable. It is reasonable because the IRLS coding in R. The runtime would improve the commented Rcpp code in the src file. You can also check https://github.com/y1zhong/LogisticRegRcpp for more reference.
